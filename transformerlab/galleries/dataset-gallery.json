[
  {
    "id": 1,
    "name": "Alpaca Data",
    "type": "Conversational",
    "huggingfacerepo": "tatsu-lab/alpaca",
    "description": "Contains 52K instruction-following data used for fine-tuning the Alpaca model.",
    "size": "24200000",
    "license": "GPL"
  },
  {
    "id": 2,
    "name": "Open Orca",
    "type": "Conversational",
    "huggingfacerepo": "Open-Orca/OpenOrca",
    "description": "Rich collection of augmented FLAN data aligns, as best as possible, with the distributions outlined in the Orca paper. It has been instrumental in generating high-performing model checkpoints and serves as a valuable resource for all NLP researchers and developers!",
    "size": "4224200000",
    "license": "GPL"
  },
  {
    "id": 3,
    "name": "Salesforce DialogStudio",
    "type": "Conversational",
    "website": "https://github.com/salesforce/DialogStudio",
    "huggingfacerepo": "Salesforce/dialogstudio",
    "description": "DialogStudio is a large collection and unified dialog datasets. The figure below provides a summary of the general statistics associated with DialogStudio. DialogStudio unified each dataset while preserving its original information, and this aids in supporting research on both individual datasets and Large Language Model (LLM) training.",
    "size": "330000000",
    "license": "GPL"
  },
  {
    "id": 4,
    "name": "Databricks Dolly 15k",
    "type": "Conversational",
    "huggingfacerepo": "databricks/databricks-dolly-15k",
    "description": "databricks-dolly-15k is an open source dataset of instruction-following records generated by thousands of Databricks employees in several of the behavioral categories outlined in the InstructGPT paper, including brainstorming, classification, closed QA, generation, information extraction, open QA, and summarization.",
    "size": "13160000",
    "license": "GPL"
  },
  {
    "id": 5,
    "name": "Clinical Dataset",
    "type": "Summarization",
    "huggingfacerepo": "Elfsong/ClinicalDataset",
    "description": "The training set consists of 1,201 pairs of conversations and associated section headers and contents. The validation set consists of 100 pairs of conversations and their summaries.",
    "size": "2400000",
    "license": "GPL"
  },
  {
    "id": 6,
    "name": "OpenChat ShareGPT GPT-4",
    "type": "Conversational",
    "huggingfacerepo": "openchat/openchat_sharegpt4_dataset",
    "description": "OpenChat is a series of open-source language models based on supervised fine-tuning (SFT). We leverage the ~80k ShareGPT conversations with a conditioning strategy and weighted loss to achieve remarkable performance despite our simple methods. Our final vision is to develop a high-performance, open-source, and commercially available large language model, and we are continuously making progress.",
    "size": "24200000",
    "license": "GPL"
  },
  {
    "id": 7,
    "name": "SAMSum Corpus",
    "type": "Summarization",
    "huggingfacerepo": "samsum",
    "description": "The SAMSum dataset contains about 16k messenger-like conversations with summaries. Conversations were created and written down by linguists fluent in English. Linguists were asked to create conversations similar to those they write on a daily basis, reflecting the proportion of topics of their real-life messenger convesations. The style and register are diversified - conversations could be informal, semi-formal or formal, they may contain slang words, emoticons and typos. Then, the conversations were annotated with summaries. It was assumed that summaries should be a concise brief of what people talked about in the conversation in third person. The SAMSum dataset was prepared by Samsung R&D Institute Poland and is distributed for research purposes (non-commercial licence: CC BY-NC-ND 4.0).",
    "size": 6.5e6,
    "license": "GPL"
  },
  {
    "id": 8,
    "name": "Wikipedia (20220301.en)",
    "type": "Text-Generation",
    "huggingfacerepo": "wikipedia",
    "description": "Wikipedia dataset containing cleaned articles of all languages. The datasets are built from the Wikipedia dump (https://dumps.wikimedia.org/) with one split per language. Each example contains the content of one full Wikipedia article with cleaning to strip markdown and unwanted sections (references, etc.).",
    "size": 20598313936,
    "license": "GPL",
    "dataset_config": "20220301.en"
  },
  {
    "id": 9,
    "name": "PKU-SafeRLHF",
    "type": "Text-Generation",
    "huggingfacerepo": "PKU-Alignment/PKU-SafeRLHF",
    "description": "The preference dataset consists of 30k+ expert comparison data. Each entry in this dataset includes two responses to a question, along with safety meta-labels and preferences for both responses, taking into consideration their helpfulness and harmlessness. The dataset is designed to help train models to generate safe and helpful responses.",
    "size": 330438,
    "license": "CC BY-NC 4.0"
  },
  {
    "id": 10,
    "name": "Nectar",
    "type": "Conversational",
    "huggingfacerepo": "berkeley-nest/Nectar",
    "description": "Nectar is the first high-quality 7-wise comparison dataset, generated through GPT-4-based ranking. Nectar contains diverse chat prompts, high-quality and diverse responses, and accurate ranking labels. Nectar's prompts are an amalgamation of diverse sources, including lmsys-chat-1M, ShareGPT, Antropic/hh-rlhf, UltraFeedback, Evol-Instruct, and Flan. ",
    "size": 182954,
    "license": "APACHE-2.0"
  },
  {
    "id": 11,
    "name": "OpenHermes",
    "type": "Text-Generation",
    "huggingfacerepo": "teknium/openhermes",
    "description": "The OpenHermes dataset is composed of 242,000 entries of primarily GPT-4 generated data.",
    "size": 242831,
    "license": "GPL"
  },
  {
    "id": 12,
    "name": "MetaMathQA",
    "type": "Conversational",
    "huggingfacerepo": "meta-math/MetaMathQA",
    "description": "MetaMath-Mistral-7B is fully fine-tuned on the MetaMathQA datasets and based on the powerful Mistral-7B model. It is glad to see using MetaMathQA datasets and changing the base model from llama-2-7B to Mistral-7b can boost the GSM8K performance from 66.5 to 77.7.",
    "size": 395000,
    "license": "MIT"
  }
]
