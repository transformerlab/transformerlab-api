{
  "name": "Common EleutherAI LM Evaluation Harness",
  "uniqueId": "common-eleuther-ai-lm-eval-harness",
  "description": "A harness for evaluating language models on the most common tasks.",
  "plugin-format": "python",
  "type": "evaluator",
  "evalsType": "model",
  "version": "0.1.11",
  "git": "https://github.com/EleutherAI/lm-evaluation-harness",
  "url": "https://github.com/EleutherAI/lm-evaluation-harness",
  "supported_hardware_architectures": ["cuda", "amd"],
  "files": ["main.py", "setup.sh"],
  "_dataset": false,
  "setup-script": "setup.sh",
  "parameters": {
    "tasks": {
      "title": "Task",
      "type": "string",
      "enum": [
        "arc_easy",
        "arc_challenge",
        "hellaswag",
        "leaderboard_bbh",
        "mmlu",
        "piqa",
        "winogrande",
        "mmlu_abstract_algebra",
        "mmlu_anatomy",
        "mmlu_astronomy",
        "mmlu_business_ethics",
        "mmlu_clinical_knowledge",
        "mmlu_college_biology",
        "mmlu_college_chemistry",
        "mmlu_college_computer_science",
        "mmlu_college_mathematics",
        "mmlu_college_medicine",
        "mmlu_college_physics",
        "mmlu_computer_security",
        "mmlu_conceptual_physics",
        "mmlu_continuation_abstract_algebra",
        "mmlu_continuation_anatomy",
        "mmlu_continuation_astronomy",
        "mmlu_continuation_business_ethics",
        "mmlu_continuation_clinical_knowledge",
        "mmlu_continuation_college_biology",
        "mmlu_continuation_college_chemistry",
        "mmlu_continuation_college_computer_science",
        "mmlu_continuation_college_mathematics",
        "mmlu_continuation_college_medicine",
        "mmlu_continuation_college_physics",
        "mmlu_continuation_computer_security",
        "mmlu_continuation_conceptual_physics",
        "mmlu_continuation_econometrics",
        "mmlu_continuation_electrical_engineering",
        "mmlu_continuation_elementary_mathematics",
        "mmlu_continuation_formal_logic",
        "mmlu_continuation_global_facts",
        "mmlu_continuation_high_school_biology",
        "mmlu_continuation_high_school_chemistry",
        "mmlu_continuation_high_school_computer_science",
        "mmlu_continuation_high_school_european_history",
        "mmlu_continuation_high_school_geography",
        "mmlu_continuation_high_school_government_and_politics",
        "mmlu_continuation_high_school_macroeconomics",
        "mmlu_continuation_high_school_mathematics",
        "mmlu_continuation_high_school_microeconomics",
        "mmlu_continuation_high_school_physics",
        "mmlu_continuation_high_school_psychology",
        "mmlu_continuation_high_school_statistics",
        "mmlu_continuation_high_school_us_history",
        "mmlu_continuation_high_school_world_history",
        "mmlu_continuation_human_aging",
        "mmlu_continuation_human_sexuality",
        "mmlu_continuation_international_law",
        "mmlu_continuation_jurisprudence",
        "mmlu_continuation_logical_fallacies",
        "mmlu_continuation_machine_learning",
        "mmlu_continuation_management",
        "mmlu_continuation_marketing",
        "mmlu_continuation_medical_genetics",
        "mmlu_continuation_miscellaneous",
        "mmlu_continuation_moral_disputes",
        "mmlu_continuation_moral_scenarios",
        "mmlu_continuation_nutrition",
        "mmlu_continuation_philosophy",
        "mmlu_continuation_prehistory",
        "mmlu_continuation_professional_accounting",
        "mmlu_continuation_professional_law",
        "mmlu_continuation_professional_medicine",
        "mmlu_continuation_professional_psychology",
        "mmlu_continuation_public_relations",
        "mmlu_continuation_security_studies",
        "mmlu_continuation_sociology",
        "mmlu_continuation_us_foreign_policy",
        "mmlu_continuation_virology",
        "mmlu_continuation_world_religions",
        "mmlu_econometrics",
        "mmlu_electrical_engineering",
        "mmlu_elementary_mathematics",
        "mmlu_flan_n_shot_loglikelihood_abstract_algebra",
        "mmlu_flan_n_shot_loglikelihood_anatomy",
        "mmlu_flan_n_shot_loglikelihood_astronomy",
        "mmlu_flan_n_shot_loglikelihood_business_ethics",
        "mmlu_flan_n_shot_loglikelihood_clinical_knowledge",
        "mmlu_flan_n_shot_loglikelihood_college_biology",
        "mmlu_flan_n_shot_loglikelihood_college_chemistry",
        "mmlu_flan_n_shot_loglikelihood_college_computer_science",
        "mmlu_flan_n_shot_loglikelihood_college_mathematics",
        "mmlu_flan_n_shot_loglikelihood_college_medicine",
        "mmlu_flan_n_shot_loglikelihood_college_physics",
        "mmlu_flan_n_shot_loglikelihood_computer_security",
        "mmlu_flan_n_shot_loglikelihood_conceptual_physics",
        "mmlu_flan_n_shot_loglikelihood_econometrics",
        "mmlu_flan_n_shot_loglikelihood_electrical_engineering",
        "mmlu_flan_n_shot_loglikelihood_elementary_mathematics",
        "mmlu_flan_n_shot_loglikelihood_formal_logic",
        "mmlu_flan_n_shot_loglikelihood_global_facts",
        "mmlu_flan_n_shot_loglikelihood_high_school_biology",
        "mmlu_flan_n_shot_loglikelihood_high_school_chemistry",
        "mmlu_flan_n_shot_loglikelihood_high_school_computer_science",
        "mmlu_flan_n_shot_loglikelihood_high_school_european_history",
        "mmlu_flan_n_shot_loglikelihood_high_school_geography",
        "mmlu_flan_n_shot_loglikelihood_high_school_government_and_politics",
        "mmlu_flan_n_shot_loglikelihood_high_school_macroeconomics",
        "mmlu_flan_n_shot_loglikelihood_high_school_mathematics",
        "mmlu_flan_n_shot_loglikelihood_high_school_microeconomics",
        "mmlu_flan_n_shot_loglikelihood_high_school_physics",
        "mmlu_flan_n_shot_loglikelihood_high_school_psychology",
        "mmlu_flan_n_shot_loglikelihood_high_school_statistics",
        "mmlu_flan_n_shot_loglikelihood_high_school_us_history",
        "mmlu_flan_n_shot_loglikelihood_high_school_world_history",
        "mmlu_flan_n_shot_loglikelihood_human_aging",
        "mmlu_flan_n_shot_loglikelihood_human_sexuality",
        "mmlu_flan_n_shot_loglikelihood_international_law",
        "mmlu_flan_n_shot_loglikelihood_jurisprudence",
        "mmlu_flan_n_shot_loglikelihood_logical_fallacies",
        "mmlu_flan_n_shot_loglikelihood_machine_learning",
        "mmlu_flan_n_shot_loglikelihood_management",
        "mmlu_flan_n_shot_loglikelihood_marketing",
        "mmlu_flan_n_shot_loglikelihood_medical_genetics",
        "mmlu_flan_n_shot_loglikelihood_miscellaneous",
        "mmlu_flan_n_shot_loglikelihood_moral_disputes",
        "mmlu_flan_n_shot_loglikelihood_moral_scenarios",
        "mmlu_flan_n_shot_loglikelihood_nutrition",
        "mmlu_flan_n_shot_loglikelihood_philosophy",
        "mmlu_flan_n_shot_loglikelihood_prehistory",
        "mmlu_flan_n_shot_loglikelihood_professional_accounting",
        "mmlu_flan_n_shot_loglikelihood_professional_law",
        "mmlu_flan_n_shot_loglikelihood_professional_medicine",
        "mmlu_flan_n_shot_loglikelihood_professional_psychology",
        "mmlu_flan_n_shot_loglikelihood_public_relations",
        "mmlu_flan_n_shot_loglikelihood_security_studies",
        "mmlu_flan_n_shot_loglikelihood_sociology",
        "mmlu_flan_n_shot_loglikelihood_us_foreign_policy",
        "mmlu_flan_n_shot_loglikelihood_virology",
        "mmlu_flan_n_shot_loglikelihood_world_religions",
        "mmlu_formal_logic",
        "mmlu_global_facts",
        "mmlu_high_school_biology",
        "mmlu_high_school_chemistry",
        "mmlu_high_school_computer_science",
        "mmlu_high_school_european_history",
        "mmlu_high_school_geography",
        "mmlu_high_school_government_and_politics",
        "mmlu_high_school_macroeconomics",
        "mmlu_high_school_mathematics",
        "mmlu_high_school_microeconomics",
        "mmlu_high_school_physics",
        "mmlu_high_school_psychology",
        "mmlu_high_school_statistics",
        "mmlu_high_school_us_history",
        "mmlu_high_school_world_history",
        "mmlu_human_aging",
        "mmlu_human_sexuality",
        "mmlu_international_law",
        "mmlu_jurisprudence",
        "mmlu_logical_fallacies",
        "mmlu_machine_learning",
        "mmlu_management",
        "mmlu_marketing",
        "mmlu_medical_genetics",
        "mmlu_miscellaneous",
        "mmlu_moral_disputes",
        "mmlu_moral_scenarios",
        "mmlu_nutrition",
        "mmlu_philosophy",
        "mmlu_prehistory",
        "mmlu_professional_accounting",
        "mmlu_professional_law",
        "mmlu_professional_medicine",
        "mmlu_professional_psychology",
        "mmlu_public_relations",
        "mmlu_security_studies",
        "mmlu_sociology",
        "mmlu_us_foreign_policy",
        "mmlu_virology",
        "mmlu_world_religions",
        "hellaswag_ar",
        "hellaswag_bn",
        "hellaswag_ca",
        "hellaswag_da",
        "hellaswag_de",
        "hellaswag_es",
        "hellaswag_eu",
        "hellaswag_fr",
        "hellaswag_gu",
        "hellaswag_hi",
        "hellaswag_hr",
        "hellaswag_hu",
        "hellaswag_hy",
        "hellaswag_id",
        "hellaswag_it",
        "hellaswag_kn",
        "hellaswag_ml",
        "hellaswag_mr",
        "hellaswag_ne",
        "hellaswag_nl",
        "hellaswag_pt",
        "hellaswag_ro",
        "hellaswag_ru",
        "hellaswag_sk",
        "hellaswag_sr",
        "hellaswag_sv",
        "hellaswag_ta",
        "hellaswag_te",
        "hellaswag_uk",
        "hellaswag_vi",
        "arc_es",
        "arc_eu",
        "arc_fr",
        "arc_gu",
        "arc_hi",
        "arc_hr",
        "arc_hu",
        "arc_hy",
        "arc_id",
        "arc_it",
        "arc_kn",
        "arc_ml",
        "arc_mr",
        "arc_ne",
        "arc_nl",
        "arc_pt",
        "arc_ro",
        "arc_ru",
        "arc_sk",
        "arc_sr",
        "arc_sv",
        "arc_ta",
        "arc_te",
        "arc_uk",
        "arc_vi",
        "arc_zh",
        "arc_challenge_mt_da",
        "arc_challenge_mt_de",
        "arc_challenge_mt_el",
        "arc_challenge_mt_es",
        "arc_challenge_mt_fi",
        "arc_challenge_mt_hu",
        "arc_challenge_mt_is",
        "arc_challenge_mt_it",
        "arc_challenge_mt_nb",
        "arc_challenge_mt_pl",
        "arc_challenge_mt_pt",
        "arc_challenge_mt_sv",
        "leaderboard_bbh_boolean_expressions",
        "leaderboard_bbh_causal_judgement",
        "leaderboard_bbh_date_understanding",
        "leaderboard_bbh_disambiguation_qa",
        "leaderboard_bbh_formal_fallacies",
        "leaderboard_bbh_geometric_shapes",
        "leaderboard_bbh_hyperbaton",
        "leaderboard_bbh_logical_deduction_five_objects",
        "leaderboard_bbh_logical_deduction_seven_objects",
        "leaderboard_bbh_logical_deduction_three_objects",
        "leaderboard_bbh_movie_recommendation",
        "leaderboard_bbh_navigate",
        "leaderboard_bbh_object_counting",
        "leaderboard_bbh_penguins_in_a_table",
        "leaderboard_bbh_reasoning_about_colored_objects",
        "leaderboard_bbh_ruin_names",
        "leaderboard_bbh_salient_translation_error_detection",
        "leaderboard_bbh_snarks",
        "leaderboard_bbh_sports_understanding",
        "leaderboard_bbh_temporal_sequences",
        "leaderboard_bbh_tracking_shuffled_objects_five_objects",
        "leaderboard_bbh_tracking_shuffled_objects_seven_objects",
        "leaderboard_bbh_tracking_shuffled_objects_three_objects",
        "leaderboard_bbh_web_of_lies",
        "piqa_ar",
        "piqa_ca",
        "piqa_eu"
      ]
    },
    "limit": {
      "title": "Number of samples (Enter a floating point between 0 and 1. Set as 1 to get all samples)",
      "type": ["number"],
      "minimum": 0.0,
      "default": 1.0,
      "maximum": 1.0,
      "multipleOf": 0.05
    }
  },
  "parameters_ui": {
    "tasks": {
      "ui:help": "Select the task you want to run from the EleutherAI Harness",
      "ui:widget": "AutoCompleteWidget"
    },
    "limit": {
      "ui:help": "Select the fraction of samples you want to run from the EleutherAI Harness task",
      "ui:widget": "RangeWidget"
    }
  }
}
