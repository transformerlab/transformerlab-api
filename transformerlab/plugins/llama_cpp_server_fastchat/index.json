{
    "name": "Llama-cpp Server",
    "uniqueId": "llama_cpp_server_fastchat",
    "description": "Runs llama-cpp-python server that can run GGUF models that work well on CPU only machines.",
    "plugin-format": "python",
    "type": "loader",
    "model_architectures": [
        "GGUF"
    ],
    "files": [
        "main.py",
        "setup.sh"
    ],
    "setup-script": "setup.sh",
    "parameters": []
}