{
    "name": "vLLM Server",
    "uniqueId": "vllm_server",
    "description": "vLLM is a fast and easy-to-use library for LLM inference and serving.",
    "plugin-format": "python",
    "type": "loader",
    "version": "1.0.8",
    "model_architectures": [
        "CohereForCausalLM",
        "FalconForCausalLM",
        "GemmaForCausalLM",
        "GPTBigCodeForCausalLM",
        "LlamaForCausalLM",
        "MistralForCausalLM",
        "MixtralForCausalLM",
        "PhiForCausalLM",
        "Phi3ForCausalLM",
        "Qwen2ForCausalLM"
    ],
    "files": [
        "main.py",
        "setup.sh"
    ],
    "setup-script": "setup.sh",
    "parameters": {
        "max-model-len": {
            "title": "Maximum Model Length",
            "type": "integer"
        },
        "gpu_memory_utilization": {
            "title": "GPU Memory Utilization",
            "type": "number",
            "default": 0.9,
            "minimum": 0.0,
            "maximum": 1.0
        },
        "tensor-parallel-size": {
            "title": "Number of GPUs",
            "type": "integer",
            "default": 1,
            "minimum": 1
        }
    }
}
