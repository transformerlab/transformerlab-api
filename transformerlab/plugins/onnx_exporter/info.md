# ONNX Exporter Plugin

## Overview
The **ONNXâ€¯Exporter** converts a ðŸ¤—â€¯Transformers model to the **ONNX** format so you can run efficient inference with ONNXâ€¯Runtime on CPUs, GPUs (CUDAâ€¯&â€¯AMD), or Appleâ€¯Silicon (MLX).  
It can also apply **dynamic INT8 quantization** in one step, giving smaller models and faster inference with minimal impact on accuracy.

---

## Supported Model Architectures
- CohereForCausalLM
- FalconForCausalLM
- LlamaForCausalLM
- Gemmaâ€¯/â€¯Gemma2ForCausalLM
- GPTâ€‘Jâ€¯ForCausalLM
- MistralForCausalLM
- MixtralForCausalLM
- Phiâ€¯/â€¯Phi3ForCausalLM
- Qwen2â€¯/â€¯Qwen3â€¯(+â€¯Qwen3Moe)â€¯ForCausalLM  
*(and any other models compatible with Optimumâ€™s ONNX exporter)*

## Parameters

| Parameter | Type | Default | Description |
|-----------|------|---------|-------------|
| **output_model_id** | string | *none* | Folder name (or HF repo name) where the exported ONNX model will be saved. |
| **opset** | integer | **17** | ONNX opset version. Keep the default unless you need an older opset for a specific runtime. |
| **quantize** | boolean | **false** | Apply dynamicâ€¯INT8 quantization after export via Optimum. Produces a smallerâ€¯`.onnx` file and can speed up inference. |

*(The plugin also uses the model selected in your experiment via `model_name`.)*

---
