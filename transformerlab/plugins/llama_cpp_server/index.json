{
  "name": "Llama-cpp Server",
  "uniqueId": "llama_cpp_server",
  "description": "Runs llama-cpp-python server that can run GGUF models that work well on CPU only machines.",
  "plugin-format": "python",
  "type": "loader",
  "version": "0.1.11",
  "model_architectures": ["GGUF"],
  "supported_hardware_architectures": ["cpu", "cuda", "mlx", "amd"],
  "files": ["main.py", "setup.sh"],
  "setup-script": "setup.sh",
  "parameters": {
    "n_gpu_layers": {
      "title": "Number of Models Layers to Offload to GPU (-1 for all, 'auto' for automatic selection)",
      "type": "string",
      "default": "auto"
    }
  }
}
