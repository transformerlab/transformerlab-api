{
  "name": "LLM-as-Judge Evaluator",
  "uniqueId": "deepeval_llm_judge",
  "description": "Using LLMs as Judges for evaluating outputs of other LLMs",
  "plugin-format": "python",
  "evalsType": "dataset",
  "type": "evaluator",
  "version": "0.2.14",
  "git": "https://github.com/confident-ai/deepeval",
  "url": "https://github.com/confident-ai/deepeval",
  "files": ["main.py", "setup.sh"],
  "_dataset": true,
  "_dataset_display_message": "Please upload a dataset file with columns: 'input', 'output', 'expected_output'. The context column is optional if using metrics which don't require it.",
  "setup-script": "setup.sh",
  "supported_hardware_architectures": ["cpu", "cuda", "mlx", "amd"],
  "parameters": {
    "generation_model": {
      "title": "Model which will act as Judge LLM",
      "type": "string"
    },
    "predefined_tasks": {
      "title": "Evaluation Metric",
      "type": "string",
      "enum": [
        "Bias",
        "Toxicity",
        "Faithfulness",
        "Hallucination",
        "Answer Relevancy",
        "Contextual Precision",
        "Contextual Recall",
        "Contextual Relevancy"
      ]
    },
    "tasks": {
      "type": "array",
      "title": "GEval Tasks",
      "description": "Define custom evaluation tasks for evaluation",
      "default": [],
      "items": {
        "type": "object",
        "properties": {
          "name": {
            "type": "string",
            "title": "Evaluation Name"
          },
          "description": {
            "type": "string",
            "title": "Text Description of the Eval providing step by step descriptions for the eval"
          },
          "evaluation_steps": {
            "type": "string",
            "title": "Evaluation Steps",
            "description": "Detailed steps for the evaluation process"
          },
          "include_context": {
            "type": "string",
            "title": "Include Context while evaluating?",
            "enum": ["Yes", "No"],
            "default": "No"
          }
        }
      }
    },
    "limit": {
      "title": "Fraction of samples",
      "type": ["number"],
      "minimum": 0.0,
      "default": 1.0,
      "maximum": 1.0,
      "multipleOf": 0.1
    },
    "dataset_split": {
      "title": "Dataset Split",
      "type": "string",
      "default": "train"
    }
  },
  "parameters_ui": {
    "predefined_tasks": {
      "ui:help": "Select one or more evaluation metrics from the drop-down list",
      "ui:widget": "AutoCompleteWidget"
    },
    "generation_model": {
      "ui:help": "Select the LLM model to be used as Judge",
      "ui:widget": "ModelProviderWidget",
      "ui:options": {
        "multiple": false
      }
    },
    "limit": {
      "ui:help": "Enter a fraction of samples to evaluate from your data. Set as 1 to get all samples",
      "ui:widget": "RangeWidget"
    },
    "geval_name": {
      "ui:help": "Enter the name of the criteria. To be used only for GEval"
    },
    "geval_context": {
      "ui:help": "Enter the description of the criteria. To be used only for GEval"
    },
    "context_geval": {
      "ui:help": "Select if the `context` field should be considered in the dataset. To be used only for GEval"
    },
    "tasks": {
      "ui:help": "For each task, provide a clear introduction (what the evaluation is about) and either detailed evaluation criteria in the description field or step-by-step evaluation steps (or both). Example: Name: 'Factual Accuracy', Description: 'Evaluate if the output is factually correct based on the provided context. Consider only verifiable facts.' For more details and best practices, see: https://www.confident-ai.com/blog/g-eval-the-definitive-guide",
      "ui:widget": "GEvalTasksWidget"
    },
    "dataset_split": {
      "ui:help": "Select the dataset split to evaluate the model on"
    }
  }
}
