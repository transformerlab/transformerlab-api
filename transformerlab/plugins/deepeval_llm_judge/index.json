{
    "name": "DeepEval Evaluations (LLM-as-Judge)",
    "uniqueId": "deepeval_llm_judge",
    "description": "Using LLMs as Judges for evaluating outputs of other LLMs",
    "plugin-format": "python",
    "type": "evaluator",
    "version": "0.2.6",
    "git": "https://github.com/confident-ai/deepeval",
    "url": "https://github.com/confident-ai/deepeval",
    "files": [
        "main.py",
        "setup.sh"
    ],
    "_dataset": true,
    "_dataset_display_message": "Please upload a dataset file with columns: 'input', 'output', 'expected_output'. The context column is optional if using metrics which don't require it.",
    "setup-script": "setup.sh",
    "parameters": {
        "tasks": {
            "title": "Evaluation Metric",
            "type": "string",
            "enum": [
                "Bias",
                "Toxicity",
                "Faithfulness",
                "Hallucination",
                "Answer Relevancy",
                "Contextual Precision",
                "Contextual Recall",
                "Contextual Relevancy",
                "Custom (GEval)"
            ]
        },
        "generation_model": {
            "title": "LLM-as-Judge Model (Model to be used as Judge. Select `local` to use the local model running)",
            "type": "string"
        },
        "limit": {
            "title": "Fraction of samples",
            "type": [
              "number"
            ],
            "minimum": 0.0,
            "default": 1.0,
            "maximum": 1.0,
            "multipleOf": 0.1
          },
        "geval_name": {
            "title": "Criteria Name (Only for GEval)",
            "type": "string"
        },
        "geval_criteria": {
            "title": "Criteria Description (Only for GEval)",
            "type": "string"
        },
        "context_geval": {
            "title": "Should `context` field be considered in dataset? (Only for GEval)",
            "type": "boolean",
            "default": false,
            "required": true
        }
    },
    "parameters_ui": {
        "tasks": {
            "ui:help": "Select an evaluation metric from the drop-down list",
            "ui:widget": "AutoCompleteWidget"
        },
        "generation_model": {
            "ui:help": "Select the LLM model to be used as Judge",
            "ui:widget": "ModelProviderWidget",
            "ui:options": {
                "multiple": false
            }
        },
        "limit": {
            "ui:help": "Enter a fraction of samples to evaluate from your data. Set as 1 to get all samples",
            "ui:widget": "RangeWidget"
        },
        "geval_name": {
            "ui:help": "Enter the name of the criteria. To be used only for GEval"
        },
        "geval_context": {
            "ui:help": "Enter the description of the criteria. To be used only for GEval"
        },
        "context_geval": {
            "ui:help": "Select if the `context` field should be considered in the dataset. To be used only for GEval"
        }
    }
}