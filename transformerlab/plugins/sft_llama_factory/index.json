{
  "name": "SFT Training -- Llama Factory",
  "uniqueId": "sft_llama_factory",
  "description": "An implementation of Supervised Finetuning using Llama Factory. This plugin is not ready for production use -- it only implements a demo finetune right now. Requires a GPU.",
  "plugin-format": "python",
  "type": "trainer",
  "version": "0.0.2",
  "model_architectures": [
    "LlamaForCausalLM",
    "MistralForCausalLM",
    "MixtralForCausalLM",
    "PhiForCausalLM",
    "GemmaForCausalLM",
    "Qwen2ForCausalLM"
  ],
  "files": ["main.py", "setup.sh"],
  "setup-script": "setup.sh",
  "training_template_format": "alpaca",
  "parameters": {
    "maximum_sequence_length": {
      "title": "Maximum Sequence Length",
      "type": "integer",
      "default": 2048,
      "minimum": 1,
      "maximum": 4096
    },
    "learning_rate": {
      "title": "Learning Rate",
      "type": "number",
      "default": 0.001,
      "minimum": 1e-6,
      "maximum": 1e6
    },
    "num_train_epochs": {
      "title": "Number of Training Epochs",
      "type": "integer",
      "default": 1,
      "minimum": 1,
      "maximum": 24
    },
    "max_steps": {
      "title": "Max Steps (-1 means no limit)",
      "type": "integer",
      "default": -1
    },
    "lora_r": {
      "title": "Lora R",
      "type": "number",
      "minimum": 4,
      "maximum": 64,
      "multipleOf": 4,
      "default": 16
    },
    "lora_alpha": {
      "title": "Lora Alpha",
      "type": "number",
      "minimum": 4,
      "maximum": 128,
      "multipleOf": 4,
      "default": 32
    },
    "lora_dropout": {
      "title": "Lora Dropout",
      "type": "number",
      "minimum": 0.05,
      "maximum": 0.9,
      "default": 0.05
    },
    "adaptor_name": {
      "title": "Adaptor Name",
      "type": "string",
      "required": true,
      "default": "adaptor"
    }
  },
  "parameters_ui": {
    "maximum_sequence_length": {
      "ui:help": "Maximum sequence length for the model. Longer sequences will be truncated. Keep lower to save memory."
    },
    "lora_r": {
      "ui:widget": "range",
      "ui:help": "Rank of the update matrices, expressed in int. Lower rank results in smaller update matrices with fewer trainable parameters."
    },
    "lora_alpha": {
      "ui:widget": "range",
      "ui:help": "LoRA scaling factor. Make it a multiple of LoRA R."
    }
  }
}
