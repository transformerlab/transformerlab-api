{
  "name": "DPO / ORPO / SIMPO RLHF - Llama Factory",
  "uniqueId": "dpo_orpo_simpo_trainer_llama_factory",
  "description": "An implementation of several Preference Optimization methods using Llama Factory.",
  "plugin-format": "python",
  "type": "trainer",
  "version": "0.0.12",
  "model_architectures": [
    "LlamaForCausalLM",
    "MistralForCausalLM",
    "MixtralForCausalLM",
    "PhiForCausalLM",
    "GemmaForCausalLM",
    "Qwen2ForCausalLM",
    "Qwen3ForCausalLM",
    "Qwen3MoeForCausalLM",
    "Phi3ForCausalLM"
  ],
  "supported_hardware_architectures": ["cuda", "amd"],
  "files": ["main.py", "setup.sh"],
  "setup-script": "setup.sh",
  "training_template_format": "none",
  "training_data_instructions": "Ensure that the dataset below has the following required columns: \"conversations\", \"chosen\", \"rejected\"",

  "parameters": {
    "pref_loss": {
      "title": "Task",
      "type": "string",
      "enum": ["dpo", "orpo", "simpo"],
      "required": true
    },
    "learning_rate": {
      "title": "Learning Rate",
      "type": "number",
      "default": 0.001,
      "minimum": 1e-6
    },
    "num_train_epochs": {
      "title": "Number of Training Epochs",
      "type": "integer",
      "default": 1,
      "minimum": 1
    },
    "max_steps": {
      "title": "Max Steps (-1 means no limit)",
      "type": "integer",
      "default": -1
    },
    "adaptor_name": {
      "title": "Adaptor Name",
      "type": "string",
      "required": true,
      "default": "dpo"
    }
  }
}
